{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlphaFold2_batch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/batch/AlphaFold2_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "#ColabFold: AlphaFold2 w/ MMseqs2 BATCH\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sokrypton/ColabFold/main/.github/ColabFold_Marv_Logo_Small.png\" height=\"256\" align=\"right\" style=\"height:256px\">\n",
        "\n",
        "Easy to use AlphaFold2 [(Jumper et al. 2021)](https://www.nature.com/articles/s41586-021-03819-2) protein structure prediction using multiple sequence alignments generated through an MMseqs2 API. For details, refer to our manuscript:\n",
        "\n",
        "[Mirdita M, Ovchinnikov S, Steinegger M. ColabFold - Making protein folding accessible to all.\n",
        "*bioRxiv*, 2021](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1) \n",
        "\n",
        "- This notebook provides basic functionality, for more advanced options (such as modeling heterocomplexes, increasing recycles, sampling, etc.) see our [advanced notebook](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb).\n",
        "- This notebook replaces the homology detection of AlphaFold2 with MMseqs2. For a comparision against the [Deepmind Colab](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) and the full [AlphaFold2](https://github.com/deepmind/alphafold) system read our [preprint](https://www.biorxiv.org/content/10.1101/2021.08.15.456425v1). \n",
        "\n",
        "\n",
        "\n",
        "**Usage**\n",
        "\n",
        "input_dir: directory with only fasta files stored in Google Drive\n",
        "\n",
        "result_dir: results will be written to the result directory in Google Drive\n",
        "\n",
        "\n",
        "<strong>For more details, see <a href=\"#Instructions\">bottom</a> of the notebook and checkout the [ColabFold GitHub](https://github.com/sokrypton/ColabFold). </strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AwvIWN3HDyUJ"
      },
      "source": [
        "#@title Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOblAo-xetgx",
        "cellView": "form"
      },
      "source": [
        "#@title Input protein sequence, then hit `Runtime` -> `Run all`\n",
        "from google.colab import files\n",
        "import os\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "input_dir = '/content/drive/MyDrive/input_fasta' #@param {type:\"string\"}\n",
        "\n",
        "result_dir = '/content/drive/MyDrive/result' #@param {type:\"string\"}\n",
        "\n",
        "# create directory\n",
        "os.makedirs(result_dir, exist_ok=True) \n",
        "\n",
        "\n",
        "\n",
        "# number of models to use\n",
        "#@markdown ---\n",
        "#@markdown ### Advanced settings\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" #@param [\"MMseqs2 (UniRef+Environmental)\", \"MMseqs2 (UniRef only)\",\"single_sequence\",\"custom\"]\n",
        "num_models = 5 #@param [1,2,3,4,5] {type:\"raw\"}\n",
        "use_msa = True if msa_mode.startswith(\"MMseqs2\") else False\n",
        "use_env = True if msa_mode == \"MMseqs2 (UniRef+Environmental)\" else False\n",
        "use_custom_msa = False\n",
        "use_amber = False #@param {type:\"boolean\"}\n",
        "use_templates = False #@param {type:\"boolean\"}\n",
        "do_not_overwite_results = True #@param {type:\"boolean\"}\n",
        "\n",
        "homooligomer = 1 \n",
        "with open(f\"run.log\", \"w\") as text_file:\n",
        "    text_file.write(\"num_models=%s\\n\" % num_models)\n",
        "    text_file.write(\"use_amber=%s\\n\" % use_amber)\n",
        "    text_file.write(\"use_msa=%s\\n\" % use_msa)\n",
        "    text_file.write(\"msa_mode=%s\\n\" % msa_mode)\n",
        "    text_file.write(\"use_templates=%s\\n\" % use_templates)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccGdbe_Pmt9",
        "cellView": "form"
      },
      "source": [
        "#@title Install dependencies\n",
        "%%bash -s $use_amber $use_msa $use_templates\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_MSA=$2\n",
        "USE_TEMPLATES=$3\n",
        "\n",
        "if [ ! -f AF2_READY ]; then\n",
        "  # install dependencies\n",
        "  pip -q install biopython dm-haiku ml-collections py3Dmol\n",
        "  wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
        "\n",
        "  # download model\n",
        "  if [ ! -d \"alphafold/\" ]; then\n",
        "    git clone https://github.com/deepmind/alphafold.git --quiet\n",
        "    (cd alphafold; git checkout 0bab1bf84d9d887aba5cfb6d09af1e8c3ecbc408 --quiet)\n",
        "    mv alphafold alphafold_\n",
        "    mv alphafold_/alphafold .\n",
        "    # remove \"END\" from PDBs, otherwise biopython complains\n",
        "    sed -i \"s/pdb_lines.append('END')//\" /content/alphafold/common/protein.py\n",
        "    sed -i \"s/pdb_lines.append('ENDMDL')//\" /content/alphafold/common/protein.py\n",
        "  fi\n",
        "\n",
        "  # download model params (~1 min)\n",
        "  if [ ! -d \"params/\" ]; then\n",
        "    mkdir params\n",
        "    curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar \\\n",
        "    | tar x -C params\n",
        "  fi\n",
        "  touch AF2_READY\n",
        "fi\n",
        "# download libraries for interfacing with MMseqs2 API\n",
        "if [ ${USE_MSA} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f MMSEQ2_READY ]; then\n",
        "    apt-get -qq -y update 2>&1 1>/dev/null\n",
        "    apt-get -qq -y install jq curl zlib1g gawk 2>&1 1>/dev/null\n",
        "    touch MMSEQ2_READY\n",
        "  fi\n",
        "fi\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniconda3-latest-Linux-x86_64.sh\n",
        "    touch CONDA_READY\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign3=3.2.2 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=7.5.1 python=3.7 pdbfixer 2>&1 1>/dev/null\n",
        "  (cd /usr/local/lib/python3.7/site-packages; patch -s -p0 < /content/alphafold_/docker/openmm.patch)\n",
        "  wget -qnc https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "  mv stereo_chemical_props.txt alphafold/common/\n",
        "  touch AMBER_READY\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYApPElB30u",
        "cellView": "form"
      },
      "source": [
        "#@title Run Prediction\n",
        "\n",
        "citations = {\n",
        "\"Mirdita2021\":  \"\"\"@article{Mirdita2021,\n",
        "author = {Mirdita, Milot and Ovchinnikov, Sergey and Steinegger, Martin},\n",
        "doi = {10.1101/2021.08.15.456425},\n",
        "journal = {bioRxiv},\n",
        "title = {{ColabFold - Making Protein folding accessible to all}},\n",
        "year = {2021},\n",
        "comment = {ColabFold including MMseqs2 MSA server}\n",
        "}\"\"\",\n",
        "  \"Mitchell2019\": \"\"\"@article{Mitchell2019,\n",
        "author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},\n",
        "doi = {10.1093/nar/gkz1035},\n",
        "journal = {Nucleic Acids Res.},\n",
        "title = {{MGnify: the microbiome analysis resource in 2020}},\n",
        "year = {2019},\n",
        "comment = {MGnify database}\n",
        "}\"\"\",\n",
        "  \"Eastman2017\": \"\"\"@article{Eastman2017,\n",
        "author = {Eastman, Peter and Swails, Jason and Chodera, John D. and McGibbon, Robert T. and Zhao, Yutong and Beauchamp, Kyle A. and Wang, Lee-Ping and Simmonett, Andrew C. and Harrigan, Matthew P. and Stern, Chaya D. and Wiewiora, Rafal P. and Brooks, Bernard R. and Pande, Vijay S.},\n",
        "doi = {10.1371/journal.pcbi.1005659},\n",
        "journal = {PLOS Comput. Biol.},\n",
        "number = {7},\n",
        "title = {{OpenMM 7: Rapid development of high performance algorithms for molecular dynamics}},\n",
        "volume = {13},\n",
        "year = {2017},\n",
        "comment = {Amber relaxation}\n",
        "}\"\"\",\n",
        "  \"Jumper2021\": \"\"\"@article{Jumper2021,\n",
        "author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n",
        "doi = {10.1038/s41586-021-03819-2},\n",
        "journal = {Nature},\n",
        "pmid = {34265844},\n",
        "title = {{Highly accurate protein structure prediction with AlphaFold.}},\n",
        "year = {2021},\n",
        "comment = {AlphaFold2 + BFD Database}\n",
        "}\"\"\",\n",
        "  \"Mirdita2019\": \"\"\"@article{Mirdita2019,\n",
        "author = {Mirdita, Milot and Steinegger, Martin and S{\\\"{o}}ding, Johannes},\n",
        "doi = {10.1093/bioinformatics/bty1057},\n",
        "journal = {Bioinformatics},\n",
        "number = {16},\n",
        "pages = {2856--2858},\n",
        "pmid = {30615063},\n",
        "title = {{MMseqs2 desktop and local web server app for fast, interactive sequence searches}},\n",
        "volume = {35},\n",
        "year = {2019},\n",
        "comment = {MMseqs2 search server}\n",
        "}\"\"\",\n",
        "  \"Steinegger2019\": \"\"\"@article{Steinegger2019,\n",
        "author = {Steinegger, Martin and Meier, Markus and Mirdita, Milot and V{\\\"{o}}hringer, Harald and Haunsberger, Stephan J. and S{\\\"{o}}ding, Johannes},\n",
        "doi = {10.1186/s12859-019-3019-7},\n",
        "journal = {BMC Bioinform.},\n",
        "number = {1},\n",
        "pages = {473},\n",
        "pmid = {31521110},\n",
        "title = {{HH-suite3 for fast remote homology detection and deep protein annotation}},\n",
        "volume = {20},\n",
        "year = {2019},\n",
        "comment = {PDB70 database}\n",
        "}\"\"\",\n",
        "  \"Mirdita2017\": \"\"\"@article{Mirdita2017,\n",
        "author = {Mirdita, Milot and von den Driesch, Lars and Galiez, Clovis and Martin, Maria J. and S{\\\"{o}}ding, Johannes and Steinegger, Martin},\n",
        "doi = {10.1093/nar/gkw1081},\n",
        "journal = {Nucleic Acids Res.},\n",
        "number = {D1},\n",
        "pages = {D170--D176},\n",
        "pmid = {27899574},\n",
        "title = {{Uniclust databases of clustered and deeply annotated protein sequences and alignments}},\n",
        "volume = {45},\n",
        "year = {2017},\n",
        "comment = {Uniclust30/UniRef30 database},\n",
        "}\"\"\",\n",
        "  \"Berman2003\": \"\"\"@misc{Berman2003,\n",
        "author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},\n",
        "booktitle = {Nat. Struct. Biol.},\n",
        "doi = {10.1038/nsb1203-980},\n",
        "number = {12},\n",
        "pages = {980},\n",
        "pmid = {14634627},\n",
        "title = {{Announcing the worldwide Protein Data Bank}},\n",
        "volume = {10},\n",
        "year = {2003},\n",
        "comment = {templates downloaded from wwPDB server}\n",
        "}\"\"\",\n",
        "}\n",
        "\n",
        "to_cite = [ \"Mirdita2021\", \"Jumper2021\" ]\n",
        "if use_msa:       to_cite += [\"Mirdita2019\"]\n",
        "if use_msa:       to_cite += [\"Mirdita2017\"]\n",
        "if use_env:       to_cite += [\"Mitchell2019\"]\n",
        "if use_templates: to_cite += [\"Steinegger2019\"]\n",
        "if use_templates: to_cite += [\"Berman2003\"]\n",
        "if use_amber:     to_cite += [\"Eastman2017\"]\n",
        "\n",
        "with open(f\"cite.bibtex\", 'w') as writer:\n",
        "  for i in to_cite:\n",
        "    writer.write(citations[i])\n",
        "    writer.write(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# setup the model\n",
        "if \"model\" not in dir():\n",
        "\n",
        "  # hiding warning messages\n",
        "  import warnings\n",
        "  from absl import logging\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  warnings.filterwarnings('ignore')\n",
        "  logging.set_verbosity(\"error\")\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  from alphafold.common import protein\n",
        "  from alphafold.data import pipeline\n",
        "  from alphafold.data import templates\n",
        "  from alphafold.model import data\n",
        "  from alphafold.model import config\n",
        "  from alphafold.model import model\n",
        "  from alphafold.data.tools import hhsearch\n",
        "  from alphafold.model.tf import shape_placeholders\n",
        "  NUM_RES = shape_placeholders.NUM_RES\n",
        "  NUM_MSA_SEQ = shape_placeholders.NUM_MSA_SEQ\n",
        "  NUM_EXTRA_SEQ = shape_placeholders.NUM_EXTRA_SEQ\n",
        "  NUM_TEMPLATES = shape_placeholders.NUM_TEMPLATES\n",
        "\n",
        "  import colabfold as cf\n",
        "  # plotting libraries\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import ipywidgets\n",
        "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
        "\n",
        "\n",
        "if use_amber and \"relax\" not in dir():\n",
        "  sys.path.insert(0, '/usr/local/lib/python3.7/site-packages/')\n",
        "  from alphafold.relax import relax\n",
        "\n",
        "def mk_mock_template(query_sequence, num_temp=1):\n",
        "  ln = len(query_sequence)\n",
        "  output_templates_sequence = \"A\"*ln\n",
        "  output_confidence_scores = np.full(ln,1.0)\n",
        "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
        "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
        "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
        "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
        "  template_features = {'template_all_atom_positions': np.tile(templates_all_atom_positions[None],[num_temp,1,1,1]),\n",
        "                       'template_all_atom_masks': np.tile(templates_all_atom_masks[None],[num_temp,1,1]),\n",
        "                       'template_sequence': [f'none'.encode()]*num_temp,\n",
        "                       'template_aatype': np.tile(np.array(templates_aatype)[None],[num_temp,1,1]),\n",
        "                       'template_confidence_scores': np.tile(output_confidence_scores[None],[num_temp,1]),\n",
        "                       'template_domain_names': [f'none'.encode()]*num_temp,\n",
        "                       'template_release_date': [f'none'.encode()]*num_temp}\n",
        "  return template_features\n",
        "\n",
        "\n",
        "def mk_template(a3m_lines, template_paths):\n",
        "  template_featurizer = templates.TemplateHitFeaturizer(\n",
        "      mmcif_dir=template_paths,\n",
        "      max_template_date=\"2100-01-01\",\n",
        "      max_hits=20,\n",
        "      kalign_binary_path=\"kalign\",\n",
        "      release_dates_path=None,\n",
        "      obsolete_pdbs_path=None)\n",
        "\n",
        "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
        "\n",
        "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
        "                                                       query_pdb_code=None,\n",
        "                                                       query_release_date=None,\n",
        "                                                       hits=hhsearch_hits)\n",
        "  return templates_result.features\n",
        "\n",
        "def make_fixed_size(protein, shape_schema, msa_cluster_size, extra_msa_size,\n",
        "                   num_res, num_templates=0):\n",
        "  \"\"\"Guess at the MSA and sequence dimensions to make fixed size.\"\"\"\n",
        "\n",
        "  pad_size_map = {\n",
        "      NUM_RES: num_res,\n",
        "      NUM_MSA_SEQ: msa_cluster_size,\n",
        "      NUM_EXTRA_SEQ: extra_msa_size,\n",
        "      NUM_TEMPLATES: num_templates,\n",
        "  }\n",
        "\n",
        "  for k, v in protein.items():\n",
        "    # Don't transfer this to the accelerator.\n",
        "    if k == 'extra_cluster_assignment':\n",
        "      continue\n",
        "    shape = list(v.shape)\n",
        "    \n",
        "    schema = shape_schema[k]\n",
        "\n",
        "    assert len(shape) == len(schema), (\n",
        "        f'Rank mismatch between shape and shape schema for {k}: '\n",
        "        f'{shape} vs {schema}')\n",
        "    pad_size = [\n",
        "        pad_size_map.get(s2, None) or s1 for (s1, s2) in zip(shape, schema)\n",
        "    ]\n",
        "    padding = [(0, p - tf.shape(v)[i]) for i, p in enumerate(pad_size)]\n",
        "\n",
        "    if padding:\n",
        "      protein[k] = tf.pad(\n",
        "          v, padding, name=f'pad_to_fixed_{k}')\n",
        "      protein[k].set_shape(pad_size)\n",
        "  return {k:np.asarray(v) for k,v in protein.items()}\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
        "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()\n",
        "\n",
        "def predict_structure(prefix, feature_dict, Ls, crop_len, model_params, use_model, do_relax=False, random_seed=0):  \n",
        "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
        "  idx_res = feature_dict['residue_index']\n",
        "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
        "\n",
        "  # Run the models.\n",
        "  plddts,paes = [],[]\n",
        "  unrelaxed_pdb_lines = []\n",
        "  relaxed_pdb_lines = []\n",
        "  seq_len = feature_dict['seq_length'][0]\n",
        "  for model_name, params in model_params.items():\n",
        "    if model_name in use_model:\n",
        "      print(f\"running {model_name}\")\n",
        "      # swap params to avoid recompiling\n",
        "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
        "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
        "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
        "      model_runner.params = params\n",
        "\n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
        "      model_config = model_runner.config\n",
        "      eval_cfg = model_config.data.eval\n",
        "      crop_feats = {k:[None]+v for k,v in dict(eval_cfg.feat).items()}     \n",
        "\n",
        "      # templates models\n",
        "      if model_name == \"model_1\" or model_name == \"model_2\":\n",
        "        pad_msa_clusters = eval_cfg.max_msa_clusters - eval_cfg.max_templates\n",
        "      else:\n",
        "        pad_msa_clusters = eval_cfg.max_msa_clusters\n",
        "\n",
        "      max_msa_clusters = pad_msa_clusters\n",
        "\n",
        "      # let's try pad (num_res + X) \n",
        "      input_fix = make_fixed_size(processed_feature_dict,\n",
        "                                  crop_feats,\n",
        "                                  msa_cluster_size=max_msa_clusters, # true_msa (4, 512, 68)\n",
        "                                  extra_msa_size=5120, # extra_msa (4, 5120, 68)\n",
        "                                  num_res=crop_len, # aatype (4, 68)\n",
        "                                  num_templates=4) # template_mask (4, 4) second value\n",
        "\n",
        "      prediction_result = model_runner.predict(input_fix)\n",
        "      unrelaxed_protein = protein.from_prediction(input_fix,prediction_result)\n",
        "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
        "      plddts.append(prediction_result['plddt'][:seq_len])\n",
        "      paes_res = []\n",
        "      for i in range(seq_len):\n",
        "        paes_res.append(prediction_result['predicted_aligned_error'][i][:seq_len])\n",
        "      paes.append(paes_res)\n",
        "      if do_relax:\n",
        "        # Relax the prediction.\n",
        "        amber_relaxer = relax.AmberRelaxation(max_iterations=0,tolerance=2.39,\n",
        "                                              stiffness=10.0,exclude_residues=[],\n",
        "                                              max_outer_iterations=20)      \n",
        "        relaxed_pdb_str, _, _ = amber_relaxer.process(prot=unrelaxed_protein)\n",
        "        relaxed_pdb_lines.append(relaxed_pdb_str)\n",
        "\n",
        "  # rerank models based on predicted lddt\n",
        "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
        "  out = {}\n",
        "  print(\"reranking models based on avg. predicted lDDT\")\n",
        "  for n,r in enumerate(lddt_rank):\n",
        "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
        "\n",
        "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
        "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
        "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    if do_relax:\n",
        "      relaxed_pdb_path = f'{prefix}_relaxed_model_{n+1}.pdb'\n",
        "      with open(relaxed_pdb_path, 'w') as f: f.write(relaxed_pdb_lines[r])\n",
        "      set_bfactor(relaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r]}\n",
        "  return out\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
        "queries = []\n",
        "for filename in onlyfiles:\n",
        "    extension = os.path.splitext(filename)[1]\n",
        "    jobname=os.path.splitext(filename)[0]\n",
        "    filepath = input_dir+\"/\"+filename\n",
        "    with open(filepath) as f:\n",
        "      input_fasta_str = f.read()\n",
        "    (seqs, header) = pipeline.parsers.parse_fasta(input_fasta_str)\n",
        "    query_sequence = seqs[0]\n",
        "    queries.append((jobname, query_sequence, extension))\n",
        "# sort by seq. len    \n",
        "queries.sort(key=lambda t: len(t[1]))\n",
        "import math\n",
        "crop_len = math.ceil(len(queries[0][1]) * 1.1)\n",
        "\n",
        "### run\n",
        "for (jobname, query_sequence, extension) in queries:\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "  if len(query_sequence) > crop_len:\n",
        "    crop_len = math.ceil(len(query_sequence) * 1.1)\n",
        "  print(\"Running: \"+jobname)\n",
        "  if do_not_overwite_results == True and os.path.isfile(result_dir+\"/\"+jobname+\".result.zip\"):\n",
        "    continue\n",
        "  if use_templates:\n",
        "    try:  \n",
        "      a3m_lines, template_paths = cf.run_mmseqs2(query_sequence, jobname, use_env, use_templates=True)\n",
        "    except:\n",
        "      print(jobname+\" cound not be processed\")\n",
        "      continue  \n",
        "    if template_paths is None:\n",
        "      template_features = mk_mock_template(query_sequence, 100)\n",
        "    else:\n",
        "      template_features = mk_template(a3m_lines, template_paths)\n",
        "    if extension.lower() == \".a3m\":\n",
        "      a3m_lines = \"\".join(open(input_dir+\"/\"+a3m_file,\"r\").read())\n",
        "  else:\n",
        "    if extension.lower() == \".a3m\":\n",
        "      a3m_lines = \"\".join(open(input_dir+\"/\"+a3m_file,\"r\").read())\n",
        "    else:\n",
        "      try:  \n",
        "        a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
        "      except:\n",
        "        print(jobname+\" cound not be processed\")\n",
        "        continue\n",
        "    template_features = mk_mock_template(query_sequence, 100)\n",
        "\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "    text_file.write(a3m_lines)\n",
        "  # parse MSA\n",
        "  msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)\n",
        "\n",
        "  #@title Gather input features, predict structure\n",
        "  from string import ascii_uppercase\n",
        "\n",
        "  # collect model weights\n",
        "  use_model = {}\n",
        "  if \"model_params\" not in dir(): model_params = {}\n",
        "  for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
        "    use_model[model_name] = True\n",
        "    if model_name not in model_params:\n",
        "      model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
        "      if model_name == \"model_1\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
        "      if model_name == \"model_3\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
        "\n",
        "\n",
        "  msas = [msa]\n",
        "  deletion_matrices = [deletion_matrix]\n",
        "  try:\n",
        "  # gather features\n",
        "    feature_dict = {\n",
        "        **pipeline.make_sequence_features(sequence=query_sequence,\n",
        "                                          description=\"none\",\n",
        "                                          num_res=len(query_sequence)),\n",
        "        **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
        "        **template_features\n",
        "    }\n",
        "  except:\n",
        "    print(jobname+\" cound not be processed\")\n",
        "    continue\n",
        "\n",
        "  \n",
        "  outs = predict_structure(jobname, feature_dict,\n",
        "                           Ls=[len(query_sequence)], crop_len=crop_len,\n",
        "                           model_params=model_params, use_model=use_model,\n",
        "                           do_relax=use_amber)\n",
        "\n",
        "  # gather MSA info\n",
        "  deduped_full_msa = list(dict.fromkeys(msa))\n",
        "  msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "  seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
        "  seqid_sort = seqid.argsort() #[::-1]\n",
        "  non_gaps = (msa_arr != \"-\").astype(float)\n",
        "  non_gaps[non_gaps == 0] = np.nan\n",
        "\n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(14,4),dpi=100)\n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
        "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "             interpolation='nearest', aspect='auto',\n",
        "             cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
        "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "  plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
        "  plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
        "  plt.colorbar(label=\"Sequence identity to query\",)\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.ylabel(\"Sequences\")\n",
        "\n",
        "  ##################################################################\n",
        "  plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
        "  for model_name,value in outs.items():\n",
        "    plt.plot(value[\"plddt\"],label=model_name)\n",
        "  if homooligomer > 0:\n",
        "    for n in range(homooligomer+1):\n",
        "      x = n*(len(query_sequence)-1)\n",
        "      plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.legend()\n",
        "  plt.ylim(0,100)\n",
        "  plt.ylabel(\"Predicted lDDT\")\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
        "  ##################################################################\n",
        "\n",
        "  print(\"Predicted Alignment Error\")\n",
        "  ##################################################################\n",
        "  plt.figure(figsize=(3*num_models,2), dpi=100)\n",
        "  for n,(model_name,value) in enumerate(outs.items()):\n",
        "    plt.subplot(1,num_models,n+1)\n",
        "    plt.title(model_name)\n",
        "    plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
        "    plt.colorbar()\n",
        "  plt.savefig(jobname+\"_PAE.png\")\n",
        "  ##################################################################\n",
        "  %shell zip -FSr $result_dir\"/\"$jobname\".result.zip\" \"run.log\" \"cite.bibtex\" $a3m_file $jobname\"_\"*\"relaxed_model_\"*\".pdb\" $jobname\"_coverage_lDDT.png\" $jobname\"_PAE.png\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUBLzB3C6WN"
      },
      "source": [
        "# Instructions <a name=\"Instructions\"></a>\n",
        "**Quick start**\n",
        "1. Upload your single fasta files to a folder in your Google Drive\n",
        "2. Define path to the fold containing the fasta files (`input_dir`) define an outdir (`output_dir`)\n",
        "3. Press \"Runtime\" -> \"Run all\".\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "At the end of the job a all results `jobname.result.zip` will be uploaded to your (`output_dir`) Google Drive. Each zip contains one protein.\n",
        "\n",
        "1. PDB formatted structures sorted by avg. pIDDT. (unrelaxed and relaxed if `use_amber` is enabled).\n",
        "2. Plots of the model quality.\n",
        "3. Plots of the MSA coverage.\n",
        "4. Parameter log file.\n",
        "5. A3M formatted input MSA.\n",
        "6. BibTeX file with citations for all used tools and databases.\n",
        "\n",
        "\n",
        "**Troubleshooting**\n",
        "* Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "* Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "* Check your input sequence.\n",
        "\n",
        "**Known issues**\n",
        "* Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence.\n",
        "* Your browser can block the pop-up for downloading the result file. You can choose the `save_to_google_drive` option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "**Limitations**\n",
        "* Computing resources: Our MMseqs2 API can handle ~20-50k requests per day.\n",
        "* MSAs: MMseqs2 is very precise and sensitive but might find less hits compared to HHblits/HMMer searched against BFD or Mgnify.\n",
        "* We recommend to additionally use the full [AlphaFold2 pipeline](https://github.com/deepmind/alphafold).\n",
        "\n",
        "**Description of the plots**\n",
        "*   **Number of sequences per position** - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. The higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better.\n",
        "\n",
        "**Bugs**\n",
        "- If you encounter any bugs, please report the issue to https://github.com/sokrypton/ColabFold/issues\n",
        "\n",
        "\n",
        "**Acknowledgments**\n",
        "- We thank the AlphaFold team for developing an excellent model and open sourcing the software. \n",
        "\n",
        "- [Söding Lab](https://www.mpibpc.mpg.de/soeding) for providing the computational resources for the MMseqs2 server\n",
        "\n",
        "- Do-Yoon Kim for creating the ColabFold logo.\n",
        "\n",
        "- A colab by Sergey Ovchinnikov ([@sokrypton](https://twitter.com/sokrypton)), Milot Mirdita ([@milot_mirdita](https://twitter.com/milot_mirdita)) and Martin Steinegger ([@thesteinegger](https://twitter.com/thesteinegger)).\n"
      ]
    }
  ]
}
