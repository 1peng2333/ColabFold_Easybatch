{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlphaFold2_advanced.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc5-mbsX9PZC"
      },
      "source": [
        "# AlphaFold2_advanced\n",
        "This notebook modifies deepmind's [original notebook](https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb) to add experimental homooligomer support, option to run MMseqs2 instead of Jackhmmer for MSA generation and advanced functionality.\n",
        "\n",
        "See [ColabFold](https://github.com/sokrypton/ColabFold/) for other related notebooks\n",
        "\n",
        "**Limitations**\n",
        "- This notebook does NOT use Templates.\n",
        "- For a typical Google-Colab session, with a `16G-GPU`, the max total length is **1400 residues**. Sometimes a `12G-GPU` is assigned, in which the max length is ~1000 residues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woIxeCPygt7K",
        "cellView": "form"
      },
      "source": [
        "#@title Install software\n",
        "#@markdown Please execute this cell by pressing the _Play_ button \n",
        "#@markdown on the left.\n",
        "use_amber_relax = False #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "# os.environ['TF_FORCE_UNIFIED_MEMORY'] = '1'\n",
        "# os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '2.0'\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "import jax\n",
        "if jax.local_devices()[0].platform == 'tpu':\n",
        "  raise RuntimeError('Colab TPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "elif jax.local_devices()[0].platform == 'cpu':\n",
        "  raise RuntimeError('Colab CPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "\n",
        "from IPython.utils import io\n",
        "import subprocess\n",
        "import tqdm.notebook\n",
        "\n",
        "GIT_REPO = 'https://github.com/deepmind/alphafold'\n",
        "SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar'\n",
        "PARAMS_DIR = './alphafold/data/params'\n",
        "PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(SOURCE_URL))\n",
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "\n",
        "# if not already installed\n",
        "try:\n",
        "  total = 100 if use_amber_relax else 55\n",
        "  with tqdm.notebook.tqdm(total=total, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      if not os.path.isdir(\"alphafold\"):\n",
        "        %shell rm -rf alphafold\n",
        "        %shell git clone {GIT_REPO} alphafold\n",
        "        %shell (cd alphafold; git checkout 1e216f93f06aa04aa699562f504db1d02c3b704c --quiet)\n",
        "\n",
        "        # colabfold patches\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/protein.patch\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/config.patch\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/model.patch\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/modules.patch\n",
        "\n",
        "        # Apply multi-chain patch from Lim Heo @huhlim\n",
        "        %shell patch -u alphafold/alphafold/common/protein.py -i protein.patch\n",
        "        \n",
        "        # Apply patch to dynamically control number of recycles (idea from Ryan Kibler)\n",
        "        %shell patch -u alphafold/alphafold/model/model.py -i model.patch\n",
        "        %shell patch -u alphafold/alphafold/model/modules.py -i modules.patch\n",
        "        %shell patch -u alphafold/alphafold/model/config.py -i config.patch\n",
        "        pbar.update(4)\n",
        "\n",
        "        %shell pip3 install ./alphafold\n",
        "        pbar.update(5)\n",
        "            \n",
        "        %shell mkdir --parents \"{PARAMS_DIR}\"\n",
        "        %shell wget -O \"{PARAMS_PATH}\" \"{SOURCE_URL}\"\n",
        "        pbar.update(14)\n",
        "\n",
        "        %shell tar --extract --verbose --file=\"{PARAMS_PATH}\" \\\n",
        "          --directory=\"{PARAMS_DIR}\" --preserve-permissions\n",
        "        %shell rm \"{PARAMS_PATH}\"\n",
        "        pbar.update(27)\n",
        "\n",
        "        #######################################################################\n",
        "        %shell sudo apt install --quiet --yes hmmer\n",
        "        pbar.update(3)\n",
        "\n",
        "        # Install py3dmol.\n",
        "        %shell pip install py3dmol\n",
        "        pbar.update(1)\n",
        "\n",
        "        # Create a ramdisk to store a database chunk to make Jackhmmer run fast.\n",
        "        %shell sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "        %shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "        pbar.update(1)\n",
        "      else:\n",
        "        pbar.update(55)\n",
        "\n",
        "      if use_amber_relax:\n",
        "        if not os.path.isfile(\"stereo_chemical_props.txt\"):\n",
        "          # Install OpenMM and pdbfixer.\n",
        "          %shell rm -rf /opt/conda\n",
        "          %shell wget -q -P /tmp \\\n",
        "            https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
        "              && bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n",
        "              && rm /tmp/Miniconda3-latest-Linux-x86_64.sh\n",
        "          pbar.update(4)\n",
        "\n",
        "          PATH=%env PATH\n",
        "          %env PATH=/opt/conda/bin:{PATH}\n",
        "          %shell conda update -qy conda \\\n",
        "              && conda install -qy -c conda-forge \\\n",
        "                python=3.7 \\\n",
        "                openmm=7.5.1 \\\n",
        "                pdbfixer\n",
        "          pbar.update(40)\n",
        "\n",
        "          %shell wget -q -P /content \\\n",
        "            https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "          pbar.update(1)\n",
        "          %shell mkdir -p /content/alphafold/common\n",
        "          %shell cp -f /content/stereo_chemical_props.txt /content/alphafold/common\n",
        "\n",
        "          # Apply OpenMM patch.\n",
        "          %shell pushd /opt/conda/lib/python3.7/site-packages/ && \\\n",
        "              patch -p0 < /content/alphafold/docker/openmm.patch && \\\n",
        "              popd\n",
        "        else:\n",
        "          pbar.update(45)\n",
        "\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "########################################################################################\n",
        "# --- Python imports ---\n",
        "import colabfold as cf\n",
        "import sys\n",
        "import pickle\n",
        "if use_amber_relax:\n",
        "  sys.path.append('/opt/conda/lib/python3.7/site-packages')\n",
        "\n",
        "from urllib import request\n",
        "from concurrent import futures\n",
        "from google.colab import files\n",
        "import json\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "\n",
        "from alphafold.model import model\n",
        "from alphafold.model import config\n",
        "from alphafold.model import data\n",
        "\n",
        "from alphafold.data import parsers\n",
        "from alphafold.data import pipeline\n",
        "from alphafold.data.tools import jackhmmer\n",
        "\n",
        "from alphafold.common import protein\n",
        "\n",
        "if use_amber_relax:\n",
        "  from alphafold.relax import relax\n",
        "  from alphafold.relax import utils\n",
        "\n",
        "def run_jackhmmer(sequence, prefix):\n",
        "  pickled_msa_path = f\"{prefix}.jackhmmer.pickle\"\n",
        "  if os.path.isfile(pickled_msa_path):\n",
        "    msas_dict = pickle.load(open(pickled_msa_path,\"rb\"))\n",
        "    msas, deletion_matrices = (msas_dict[k] for k in ['msas', 'deletion_matrices'])\n",
        "    full_msa = []\n",
        "    for msa in msas:\n",
        "      full_msa += msa\n",
        "  else:\n",
        "    # --- Find the closest source ---\n",
        "    test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2021_03.fasta.1'\n",
        "    ex = futures.ThreadPoolExecutor(3)\n",
        "    def fetch(source):\n",
        "      request.urlretrieve(test_url_pattern.format(source))\n",
        "      return source\n",
        "    fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
        "    source = None\n",
        "    for f in futures.as_completed(fs):\n",
        "      source = f.result()\n",
        "      ex.shutdown()\n",
        "      break\n",
        "\n",
        "    jackhmmer_binary_path = '/usr/bin/jackhmmer'\n",
        "    dbs = []\n",
        "\n",
        "    num_jackhmmer_chunks = {'uniref90': 59, 'smallbfd': 17, 'mgnify': 71}\n",
        "    total_jackhmmer_chunks = sum(num_jackhmmer_chunks.values())\n",
        "    with tqdm.notebook.tqdm(total=total_jackhmmer_chunks, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "      def jackhmmer_chunk_callback(i):\n",
        "        pbar.update(n=1)\n",
        "\n",
        "      pbar.set_description('Searching uniref90')\n",
        "      jackhmmer_uniref90_runner = jackhmmer.Jackhmmer(\n",
        "          binary_path=jackhmmer_binary_path,\n",
        "          database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/uniref90_2021_03.fasta',\n",
        "          get_tblout=True,\n",
        "          num_streamed_chunks=num_jackhmmer_chunks['uniref90'],\n",
        "          streaming_callback=jackhmmer_chunk_callback,\n",
        "          z_value=135301051)\n",
        "      dbs.append(('uniref90', jackhmmer_uniref90_runner.query('target.fasta')))\n",
        "\n",
        "      pbar.set_description('Searching smallbfd')\n",
        "      jackhmmer_smallbfd_runner = jackhmmer.Jackhmmer(\n",
        "          binary_path=jackhmmer_binary_path,\n",
        "          database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/bfd-first_non_consensus_sequences.fasta',\n",
        "          get_tblout=True,\n",
        "          num_streamed_chunks=num_jackhmmer_chunks['smallbfd'],\n",
        "          streaming_callback=jackhmmer_chunk_callback,\n",
        "          z_value=65984053)\n",
        "      dbs.append(('smallbfd', jackhmmer_smallbfd_runner.query('target.fasta')))\n",
        "\n",
        "      pbar.set_description('Searching mgnify')\n",
        "      jackhmmer_mgnify_runner = jackhmmer.Jackhmmer(\n",
        "          binary_path=jackhmmer_binary_path,\n",
        "          database_path=f'https://storage.googleapis.com/alphafold-colab{source}/latest/mgy_clusters_2019_05.fasta',\n",
        "          get_tblout=True,\n",
        "          num_streamed_chunks=num_jackhmmer_chunks['mgnify'],\n",
        "          streaming_callback=jackhmmer_chunk_callback,\n",
        "          z_value=304820129)\n",
        "      dbs.append(('mgnify', jackhmmer_mgnify_runner.query('target.fasta')))\n",
        "\n",
        "    # --- Extract the MSAs and visualize ---\n",
        "    # Extract the MSAs from the Stockholm files.\n",
        "    # NB: deduplication happens later in pipeline.make_msa_features.\n",
        "\n",
        "    mgnify_max_hits = 501\n",
        "    msas = []\n",
        "    deletion_matrices = []\n",
        "    for db_name, db_results in dbs:\n",
        "      unsorted_results = []\n",
        "      for i, result in enumerate(db_results):\n",
        "        msa, deletion_matrix, target_names = parsers.parse_stockholm(result['sto'])\n",
        "        e_values_dict = parsers.parse_e_values_from_tblout(result['tbl'])\n",
        "        e_values = [e_values_dict[t.split('/')[0]] for t in target_names]\n",
        "        zipped_results = zip(msa, deletion_matrix, target_names, e_values)\n",
        "        if i != 0:\n",
        "          # Only take query from the first chunk\n",
        "          zipped_results = [x for x in zipped_results if x[2] != 'query']\n",
        "        unsorted_results.extend(zipped_results)\n",
        "      sorted_by_evalue = sorted(unsorted_results, key=lambda x: x[3])\n",
        "      db_msas, db_deletion_matrices, _, _ = zip(*sorted_by_evalue)\n",
        "      if db_msas:\n",
        "        if db_name == 'mgnify':\n",
        "          db_msas = db_msas[:mgnify_max_hits]\n",
        "          db_deletion_matrices = db_deletion_matrices[:mgnify_max_hits]\n",
        "        msas.append(db_msas)\n",
        "        deletion_matrices.append(db_deletion_matrices)\n",
        "        msa_size = len(set(db_msas))\n",
        "        print(f'{msa_size} Sequences Found in {db_name}')\n",
        "\n",
        "      pickle.dump({\"msas\":msas,\"deletion_matrices\":deletion_matrices},\n",
        "                  open(pickled_msa_path,\"wb\"))\n",
        "  return msas, deletion_matrices\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4JpOs6oA-QS"
      },
      "source": [
        "## Making a prediction\n",
        "\n",
        "Please paste the sequence of your protein in the text box below, then run the remaining cells via _Runtime_ > _Run after_. You can also run the cells individually by pressing the _Play_ button on the left.\n",
        "\n",
        "Note that the search against databases and the actual prediction can take some time, from minutes to hours, depending on the length of the protein and what type of GPU you are allocated by Colab (see FAQ below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rowN0bVYLe9n",
        "cellView": "form"
      },
      "source": [
        "#@title Enter the amino acid sequence to fold ⬇️\n",
        "sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK'  #@param {type:\"string\"}\n",
        "MIN_SEQUENCE_LENGTH = 16\n",
        "MAX_SEQUENCE_LENGTH = 2500\n",
        "\n",
        "# Remove all whitespaces, tabs and end lines; upper-case\n",
        "sequence = sequence.translate(str.maketrans('', '', ' \\n\\t')).upper()\n",
        "homooligomer = 1 #@param [1,2,3,4,5,6,7,8] {type:\"raw\"}\n",
        "#@markdown - Define number of copies in a homo-oligomeric assembly.\n",
        "\n",
        "full_sequence = sequence * homooligomer\n",
        "\n",
        "# prediction directory\n",
        "output_dir = 'prediction_' + cf.get_hash(full_sequence)[:5]\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"working directory: {output_dir}\")\n",
        "\n",
        "aatypes = set('ACDEFGHIKLMNPQRSTVWY')  # 20 standard aatypes\n",
        "if not set(sequence).issubset(aatypes):\n",
        "  raise Exception(f'Input sequence contains non-amino acid letters: {set(sequence) - aatypes}. AlphaFold only supports 20 standard amino acids as inputs.')\n",
        "if len(full_sequence) < MIN_SEQUENCE_LENGTH:\n",
        "  raise Exception(f'Input sequence is too short: {len(full_sequence)} amino acids, while the minimum is {MIN_SEQUENCE_LENGTH}')\n",
        "if len(full_sequence) > MAX_SEQUENCE_LENGTH:\n",
        "  raise Exception(f'Input sequence is too long: {len(full_sequence)} amino acids, while the maximum is {MAX_SEQUENCE_LENGTH}. Please use the full AlphaFold system for long sequences.')\n",
        "\n",
        "if len(full_sequence) > 1400:\n",
        "  print(f\"WARNING: For a typical Google-Colab-GPU (16G) session, the max total length is ~1300 residues. You are at {len(full_sequence)}! Run Alphafold may crash.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq5EC6ju-XPo",
        "cellView": "form"
      },
      "source": [
        "#@title Search against genetic databases\n",
        "#@markdown Once this cell has been executed, you will see\n",
        "#@markdown statistics about the multiple sequence alignment \n",
        "#@markdown (MSA) that will be used by AlphaFold. In particular, \n",
        "#@markdown you’ll see how well each residue is covered by similar \n",
        "#@markdown sequences in the MSA.\n",
        "\n",
        "#@markdown ---\n",
        "msa_method = \"mmseqs2\" #@param [\"mmseqs2\",\"jackhmmer\",\"single_sequence\",\"custom_a3m\",\"precomputed\"]\n",
        "#@markdown - `mmseqs2` - FAST method from [ColabFold](https://github.com/sokrypton/ColabFold)\n",
        "#@markdown - `jackhmmer` - default method from Deepmind (SLOW, but may find more/less sequences).\n",
        "#@markdown - `single_sequence` - use single sequence input (not recommended, unless a *denovo* design and you dont expect to find any homologous sequences)\n",
        "#@markdown - `custom_a3m` Upload custom MSA (a3m format)\n",
        "#@markdown - `precomputed` If you have previously run this notebook and saved the results,\n",
        "#@markdown you can skip this step by uploading \n",
        "#@markdown the previously generated  `prediction/msa.npz`\n",
        "cov = 0 #@param [\"0\",\"25\",\"50\",\"75\",\"90\"] {type:\"raw\"}\n",
        "#@markdown - filter to remove sequences that don't cover at least `cov` % of query. (Set to `0` to disable all fitlering.)\n",
        "\n",
        "# tmp directory\n",
        "prefix = cf.get_hash(sequence)\n",
        "os.makedirs('tmp', exist_ok=True)\n",
        "prefix = os.path.join('tmp',prefix)\n",
        "\n",
        "# --- Search against genetic databases ---\n",
        "with open('target.fasta', 'wt') as f:\n",
        "  f.write(f'>query\\n{sequence}')\n",
        "\n",
        "# Run the search against chunks of genetic databases (since the genetic\n",
        "# databases don't fit in Colab ramdisk).\n",
        "\n",
        "if msa_method == \"precomputed\":\n",
        "  print(\"upload precomputed pickled msa from previous run\")\n",
        "  pickled_msa_dict = files.upload()\n",
        "  msas_dict = pickle.loads(pickled_msa_dict[list(pickled_msa_dict.keys())[0]])\n",
        "  msas, deletion_matrices = (msas_dict[k] for k in ['msas', 'deletion_matrices'])\n",
        "\n",
        "elif msa_method == \"mmseqs2\":\n",
        "  a3m_lines = cf.run_mmseqs2(sequence, prefix, filter=True)\n",
        "  msa, deletion_matrix = parsers.parse_a3m(a3m_lines)\n",
        "  msas,deletion_matrices = [msa],[deletion_matrix]\n",
        "\n",
        "elif msa_method == \"single_sequence\":\n",
        "  msas = [[sequence]]\n",
        "  deletion_matrices = [[[0]*len(sequence)]]\n",
        "\n",
        "elif msa_method == \"custom_a3m\":\n",
        "  print(\"upload custom a3m\")\n",
        "  msa_dict = files.upload()\n",
        "  lines = msa_dict[list(msa_dict.keys())[0]].decode().splitlines()\n",
        "  a3m_lines = []\n",
        "  for line in lines:\n",
        "    line = line.replace(\"\\x00\",\"\")\n",
        "    if len(line) > 0 and not line.startswith('#'):\n",
        "      a3m_lines.append(line)\n",
        "  msa, deletion_matrix = parsers.parse_a3m(\"\\n\".join(a3m_lines))\n",
        "  msas,deletion_matrices = [msa],[deletion_matrix]\n",
        "\n",
        "  if len(msas[0][0]) != len(sequence):\n",
        "    print(\"ERROR: the length of msa does not match input sequence\")\n",
        "\n",
        "else:\n",
        "  # run jackhmmer\n",
        "  msas, deletion_matrices = run_jackhmmer(sequence, prefix)\n",
        "\n",
        "# save MSA as pickle\n",
        "pickle.dump({\"msas\":msas,\"deletion_matrices\":deletion_matrices},\n",
        "            open(os.path.join(output_dir,\"msa.pickle\"),\"wb\"))\n",
        "\n",
        "if msa_method != \"single_sequence\":\n",
        "  # filter sequences that don't cover at least % \n",
        "  msas, deletion_matrices = cf.cov_filter(msas, deletion_matrices, cov)\n",
        "      \n",
        "full_msa = []\n",
        "for msa in msas: full_msa += msa\n",
        "\n",
        "# deduplicate\n",
        "deduped_full_msa = list(dict.fromkeys(full_msa))\n",
        "total_msa_size = len(deduped_full_msa)\n",
        "if msa_method == \"mmseqs2\":\n",
        "  print(f'\\n{total_msa_size} Sequences Found in Total (after filtering)\\n')\n",
        "else:\n",
        "  print(f'\\n{total_msa_size} Sequences Found in Total\\n')\n",
        "\n",
        "msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "num_alignments, num_res = msa_arr.shape\n",
        "\n",
        "if num_alignments > 1:\n",
        "  plt.figure(figsize=(8,5),dpi=100)\n",
        "  plt.title(\"Sequence coverage\")\n",
        "  seqid = (np.array(list(sequence)) == msa_arr).mean(-1)\n",
        "  seqid_sort = seqid.argsort() #[::-1]\n",
        "  non_gaps = (msa_arr != \"-\").astype(float)\n",
        "  non_gaps[non_gaps == 0] = np.nan\n",
        "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "            interpolation='nearest', aspect='auto',\n",
        "            cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower',\n",
        "            extent=(0, msa_arr.shape[1], 0, msa_arr.shape[0]))\n",
        "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "  plt.xlim(0,msa_arr.shape[1])\n",
        "  plt.ylim(0,msa_arr.shape[0])\n",
        "  plt.colorbar(label=\"Sequence identity to query\",)\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.ylabel(\"Sequences\")\n",
        "  plt.savefig(os.path.join(output_dir,\"msa_coverage.png\"), bbox_inches = 'tight')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlGF_0IACXBl",
        "cellView": "form"
      },
      "source": [
        "#@title Run AlphaFold \n",
        "\n",
        "#@markdown ---\n",
        "use_ptm = True #@param {type:\"boolean\"}\n",
        "max_recycles = 3 #@param [3,6,12,24,48] {type:\"raw\"}\n",
        "tol = 0 #@param [0,0.1,0.5,1] {type:\"raw\"}\n",
        "#@markdown - `use_ptm` uses Deepmind's `ptm` finetuned model parameters to get PAE per structure. Disable to use the original model params.\n",
        "#@markdown - `max_recycles` controls the maximum number of times the structure is fed back into the neural network for refinement.\n",
        "#@markdown - `tol` tolerance for deciding when to stop (CA-RMS between recycles)\n",
        "samples_per_model = 1 #@param [1,2,3,4,5,6,7,8,9,10] {type:\"raw\"}\n",
        "#@markdown - Number of random_seeds to iterate through for each model. (Can help find correct solution, especially when dealing with limited (<100) number of sequences.)\n",
        "use_turbo = True #@param {type:\"boolean\"}\n",
        "relax_all = False #@param {type:\"boolean\"}\n",
        "save_pae_json = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown - `use_turbo` introduces a few modifications (compile once, swap params, adjust max_msa) to speedup and reduce memory requirements. Disable for default behavior.\n",
        "#@markdown - `relax_all` amber-relax all models. Disable to only relax the top ranked model. (Note: no models are relaxed if `use_amber_relax` is disabled.)\n",
        "\n",
        "# --- Run the model ---\n",
        "from string import ascii_uppercase\n",
        "\n",
        "def _placeholder_template_feats(num_templates_, num_res_):\n",
        "  return {\n",
        "      'template_aatype': np.zeros([num_templates_, num_res_, 22], np.float32),\n",
        "      'template_all_atom_masks': np.zeros([num_templates_, num_res_, 37, 3], np.float32),\n",
        "      'template_all_atom_positions': np.zeros([num_templates_, num_res_, 37], np.float32),\n",
        "      'template_domain_names': np.zeros([num_templates_], np.float32),\n",
        "      'template_sum_probs': np.zeros([num_templates_], np.float32),\n",
        "  }\n",
        "\n",
        "num_templates = 0\n",
        "num_res = len(sequence)\n",
        "msas_mod, deletion_matrices_mod = cf.homooliomerize(msas, deletion_matrices, homooligomer)\n",
        "\n",
        "feature_dict = {}\n",
        "feature_dict.update(pipeline.make_sequence_features(sequence*homooligomer, 'test', num_res*homooligomer))\n",
        "feature_dict.update(pipeline.make_msa_features(msas_mod, deletion_matrices=deletion_matrices_mod))\n",
        "if not use_turbo:\n",
        "  feature_dict.update(_placeholder_template_feats(num_templates, num_res*homooligomer))\n",
        "\n",
        "Ls = [num_res]*homooligomer\n",
        "feature_dict['residue_index'] = cf.chain_break(feature_dict['residue_index'], Ls)\n",
        "\n",
        "\n",
        "def parse_results(prediction_result, processed_feature_dict):\n",
        "  b_factors = prediction_result['plddt'][:,None] * prediction_result['structure_module']['final_atom_mask']\n",
        "  out = {\"unrelaxed_protein\": protein.from_prediction(processed_feature_dict, prediction_result, b_factors=b_factors),\n",
        "         \"plddt\": prediction_result['plddt'],\n",
        "         \"sco\": prediction_result['plddt'].mean(),\n",
        "         \"dists\": prediction_result[\"distogram\"][\"bin_edges\"][prediction_result[\"distogram\"][\"logits\"].argmax(-1)],\n",
        "         \"adj\": jax.nn.softmax(prediction_result[\"distogram\"][\"logits\"])[:,:,prediction_result[\"distogram\"][\"bin_edges\"] < 8].sum(-1)}\n",
        "  if \"ptm\" in prediction_result:\n",
        "    out.update({\"pae\": prediction_result['predicted_aligned_error'],\n",
        "                \"ptm\": prediction_result['ptm']})\n",
        "  return out\n",
        "\n",
        "model_names = ['model_1', 'model_2', 'model_3', 'model_4', 'model_5']\n",
        "total = len(model_names) * samples_per_model\n",
        "if use_amber_relax:\n",
        "  if relax_all: total += total\n",
        "  else: total += 1\n",
        "\n",
        "with tqdm.notebook.tqdm(total=total, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "  #######################################################################\n",
        "  # precompile model and recompile only if length changes\n",
        "  if use_turbo:\n",
        "    name = \"model_5_ptm\" if use_ptm else \"model_5\"\n",
        "    N = msa_arr.shape[0]\n",
        "    L = msa_arr.shape[1] * homooligomer\n",
        "    compiled = (N,L,use_ptm,max_recycles,tol)\n",
        "    if \"COMPILED\" in dir():\n",
        "      if COMPILED != compiled: recompile = True\n",
        "    else: recompile = True\n",
        "    if recompile:\n",
        "      cf.clear_mem(\"gpu\")\n",
        "      cfg = config.model_config(name)\n",
        "      cfg.data.common.max_extra_msa = min(N,1024)\n",
        "      cfg.data.eval.max_msa_clusters = min(N,512)\n",
        "      cfg.data.common.num_recycle = max_recycles\n",
        "      cfg.model.num_recycle = max_recycles\n",
        "      cfg.model.recycle_tol = tol\n",
        "\n",
        "      params = data.get_model_haiku_params(name,'./alphafold/data')\n",
        "      model_runner = model.RunModel(cfg, params)\n",
        "      COMPILED = compiled\n",
        "      recompile = False\n",
        "  else:\n",
        "    cf.clear_mem(\"gpu\")\n",
        "    recompile = True\n",
        "\n",
        "  # cleanup\n",
        "  if \"outs\" in dir(): del outs\n",
        "  outs = {}\n",
        "  cf.clear_mem(\"cpu\")  \n",
        "\n",
        "  #######################################################################\n",
        "  if use_turbo:\n",
        "    for seed in range(samples_per_model): # for each seed\n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=seed)\n",
        "      for num, model_name in enumerate(model_names): # for each model\n",
        "        name = model_name+\"_ptm\" if use_ptm else model_name\n",
        "        key = f\"{name}_seed_{seed}\"\n",
        "        pbar.set_description(f'Running {key}')\n",
        "        name = model_name+\"_ptm\" if use_ptm else model_name\n",
        "        model_runner.params = data.get_model_haiku_params(name, './alphafold/data')\n",
        "        prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict),\"cpu\")\n",
        "        outs[key] = parse_results(prediction_result, processed_feature_dict)\n",
        "        pbar.update(n=1)\n",
        "        print(f\"{key} recycles:{r} tol:{t:.2f} plddt:{outs[key]['sco']:.2f}\" + (f\" ptm:{outs[key]['ptm']:.2f}\" if use_ptm else \"\"))\n",
        "\n",
        "        # cleanup\n",
        "        del prediction_result\n",
        "      # cleanup\n",
        "      del processed_feature_dict\n",
        "  else:\n",
        "    for num, model_name in enumerate(model_names): # for each model\n",
        "      name = model_name+\"_ptm\" if use_ptm else model_name\n",
        "      params = data.get_model_haiku_params(name, './alphafold/data')      \n",
        "      cfg = config.model_config(name)\n",
        "      cfg.data.common.num_recycle = cfg.model.num_recycle = max_recycles\n",
        "      cfg.model.recycle_tol = tol\n",
        "      model_runner = model.RunModel(cfg, params)\n",
        "      for seed in range(samples_per_model): # for each seed\n",
        "        key = f\"{name}_seed_{seed}\"\n",
        "        pbar.set_description(f'Running {key}')\n",
        "        processed_feature_dict = model_runner.process_features(feature_dict, random_seed=seed)\n",
        "        prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict),\"cpu\")        \n",
        "        \n",
        "        outs[key] = parse_results(prediction_result, processed_feature_dict)\n",
        "        pbar.update(n=1)\n",
        "        print(f\"{key} recycles:{r} tol:{t:.2f} plddt:{outs[key]['sco']:.2f}\" + (f\" ptm:{outs[key]['ptm']:.4f}\" if use_ptm else \"\"))\n",
        "\n",
        "        # cleanup\n",
        "        del processed_feature_dict\n",
        "        del prediction_result\n",
        "\n",
        "      # cleanup\n",
        "      del model_runner\n",
        "      del cfg      \n",
        "      cf.clear_mem(\"gpu\")\n",
        "\n",
        "\n",
        "  # Find the best model according to the mean pLDDT.\n",
        "  model_rank = list(outs.keys())\n",
        "  model_rank = [model_rank[i] for i in np.argsort([outs[x][\"sco\"] for x in model_rank])[::-1]]\n",
        "\n",
        "  # Write out the prediction\n",
        "  for n,key in enumerate(model_rank):\n",
        "    prefix = f\"rank_{n+1}_{key}\" \n",
        "    pred_output_path = os.path.join(output_dir,f'{prefix}_unrelaxed.pdb')\n",
        "    pdb_lines = protein.to_pdb(outs[key][\"unrelaxed_protein\"])\n",
        "    with open(pred_output_path, 'w') as f:\n",
        "      f.write(pdb_lines)\n",
        "    if use_amber_relax:\n",
        "      pbar.set_description(f'AMBER relaxation')\n",
        "      if relax_all or n == 0:\n",
        "        amber_relaxer = relax.AmberRelaxation(\n",
        "            max_iterations=0,\n",
        "            tolerance=2.39,\n",
        "            stiffness=10.0,\n",
        "            exclude_residues=[],\n",
        "            max_outer_iterations=20)\n",
        "        relaxed_pdb_lines, _, _ = amber_relaxer.process(prot=outs[key][\"unrelaxed_protein\"])        \n",
        "        pred_output_path = os.path.join(output_dir,f'{prefix}_relaxed.pdb')\n",
        "        with open(pred_output_path, 'w') as f:\n",
        "          f.write(relaxed_pdb_lines)\n",
        "      pbar.update(n=1)\n",
        "      \n",
        "############################################################\n",
        "print(\"models ranked based on pLDDT\")\n",
        "for n,key in enumerate(model_rank):\n",
        "  print(f\"rank_{n+1}_{key} {outs[key]['sco']:.2f}\")\n",
        "  if use_ptm and save_pae_json:\n",
        "    pae = outs[key][\"pae\"]\n",
        "    max_pae = pae.max()\n",
        "    # Save pLDDT and predicted aligned error (if it exists)\n",
        "    pae_output_path = os.path.join(output_dir,f'predicted_aligned_error_rank_{n+1}_{key}.json')\n",
        "    # Save predicted aligned error in the same format as the AF EMBL DB\n",
        "    rounded_errors = np.round(np.asarray(pae), decimals=1)\n",
        "    indices = np.indices((len(rounded_errors), len(rounded_errors))) + 1\n",
        "    indices_1 = indices[0].flatten().tolist()\n",
        "    indices_2 = indices[1].flatten().tolist()\n",
        "    pae_data = json.dumps([{\n",
        "        'residue1': indices_1,\n",
        "        'residue2': indices_2,\n",
        "        'distance': rounded_errors.flatten().tolist(),\n",
        "        'max_predicted_aligned_error': max_pae.item()\n",
        "    }],\n",
        "                          indent=None,\n",
        "                          separators=(',', ':'))\n",
        "    with open(pae_output_path, 'w') as f:\n",
        "      f.write(pae_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAZj6CBZTkJM",
        "cellView": "form"
      },
      "source": [
        "#@title Display 3D structure {run: \"auto\"}\n",
        "rank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"10\"] {type:\"raw\"}\n",
        "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "\n",
        "key = model_rank[rank_num-1]\n",
        "prefix = f\"rank_{rank_num}_{key}\" \n",
        "pred_output_path = os.path.join(output_dir,f'{prefix}_relaxed.pdb')  \n",
        "if not os.path.isfile(pred_output_path):\n",
        "  pred_output_path = os.path.join(output_dir,f'{prefix}_unrelaxed.pdb') \n",
        "\n",
        "cf.show_pdb(pred_output_path, show_sidechains, show_mainchains, color, chains=homooligomer).show()\n",
        "if color == \"lDDT\": cf.plot_plddt_legend().show()  \n",
        "if use_ptm:\n",
        "  cf.plot_confidence(outs[key][\"plddt\"], outs[key][\"pae\"], Ls=Ls).show()\n",
        "else:\n",
        "  cf.plot_confidence(outs[key][\"plddt\"], Ls=Ls).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Riekgf0KQv_3",
        "cellView": "form"
      },
      "source": [
        "#@title Download prediction\n",
        "\n",
        "#@markdown Once this cell has been executed, a zip-archive with \n",
        "#@markdown the obtained prediction will be automatically downloaded \n",
        "#@markdown to your computer.\n",
        "\n",
        "# add settings file\n",
        "settings_path = os.path.join(output_dir,\"settings.txt\")\n",
        "with open(settings_path, \"w\") as text_file:\n",
        "  text_file.write(f\"sequence={sequence}\\n\")\n",
        "  text_file.write(f\"msa_method={msa_method}\\n\")\n",
        "  text_file.write(f\"homooligomer={homooligomer}\\n\")\n",
        "  text_file.write(f\"use_amber_relax={use_amber_relax}\\n\")\n",
        "  text_file.write(f\"use_turbo={use_turbo}\\n\")\n",
        "  text_file.write(f\"use_ptm={use_ptm}\\n\")\n",
        "  text_file.write(f\"cov={cov}\\n\")\n",
        "  text_file.write(f\"samples={samples_per_model}\\n\")\n",
        "  text_file.write(f\"max_recycles={max_recycles}\\n\")\n",
        "  text_file.write(f\"tol={tol}\\n\")\n",
        "  text_file.write(f\"use_templates=False\\n\")\n",
        "\n",
        "# --- Download the predictions ---\n",
        "!zip -q -r {output_dir}.zip {output_dir}\n",
        "files.download(f'{output_dir}.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzK2Wve12GCk",
        "cellView": "form"
      },
      "source": [
        "#@title Extra plots\n",
        "dpi =  100#@param {type:\"integer\"}\n",
        "\n",
        "# cf.plot_plddts([outs[k][\"plddt\"] for k in model_rank], Ls=Ls).show()\n",
        "if use_ptm:\n",
        "  print(\"predicted alignment error\")\n",
        "  cf.plot_paes([outs[k][\"pae\"] for k in model_rank],dpi=dpi).show()\n",
        "print(\"predicted contacts\")\n",
        "cf.plot_adjs([outs[k][\"adj\"] for k in model_rank],dpi=dpi).show()\n",
        "print(\"predicted distogram\")\n",
        "cf.plot_dists([outs[k][\"dists\"] for k in model_rank],dpi=dpi).show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}