{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlphaFold2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/AlphaFold2_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yBrceuFbf3"
      },
      "source": [
        "#AlphaFold2 w/ MMseqs2 Batch\n",
        "\n",
        "Easy to use version of AlphaFold 2 [(Jumper et al. 2021, Nature)](https://www.nature.com/articles/s41586-021-03819-2) a protein structure prediction pipeline, with an API hosted at the SÃ¶dinglab based on the MMseqs2 server [(Mirdita et al. 2019, Bioinformatics)](https://academic.oup.com/bioinformatics/article/35/16/2856/5280135) for the multiple sequence alignment creation. \n",
        "\n",
        "**Usage**\n",
        "\n",
        "input_dir: directory with only fasta files stored in Google Drive\n",
        "\n",
        "result_dir: results will be written to the result directory in Google Drive\n",
        "\n",
        "<strong>For detailed instructions, see <a href=\"#Instructions\">bottom</a> of notebook!</strong>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AwvIWN3HDyUJ"
      },
      "source": [
        "#@title Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOblAo-xetgx",
        "cellView": "form"
      },
      "source": [
        "#@title Input protein sequence, then hit `Runtime` -> `Run all`\n",
        "from google.colab import files\n",
        "import os\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "input_dir = '/content/drive/MyDrive/input_fasta' #@param {type:\"string\"}\n",
        "\n",
        "result_dir = '/content/drive/MyDrive/result' #@param {type:\"string\"}\n",
        "\n",
        "# number of models to use\n",
        "#@markdown ---\n",
        "#@markdown ### Advanced settings\n",
        "msa_mode = \"MMseqs2 (UniRef+Environmental)\" #@param [\"MMseqs2 (UniRef+Environmental)\", \"MMseqs2 (UniRef only)\",\"single_sequence\",\"custom\"]\n",
        "num_models = 5 #@param [1,2,3,4,5] {type:\"raw\"}\n",
        "use_msa = True if msa_mode.startswith(\"MMseqs2\") else False\n",
        "use_env = True if msa_mode == \"MMseqs2 (UniRef+Environmental)\" else False\n",
        "use_custom_msa = False\n",
        "use_amber = False #@param {type:\"boolean\"}\n",
        "use_templates = False \n",
        "use_turbo = True\n",
        "use_ptm = True\n",
        "max_recycles = 3\n",
        "tol = 0\n",
        "is_training = False \n",
        "num_samples = 1 \n",
        "num_ensemble = 1 \n",
        "show_images = False\n",
        "save_tmp_pdb = True\n",
        "rank_by = \"pLDDT\" \n",
        "save_pae_json = False\n",
        "\n",
        "homooligomer =  \"1\" \n",
        "homooligomer = re.sub(\"[:/]+\",\":\",homooligomer)\n",
        "if len(homooligomer) == 0: homooligomer = \"1\"\n",
        "homooligomer = re.sub(\"[^0-9:]\", \"\", homooligomer)\n",
        "homooligomers = [int(h) for h in homooligomer.split(\":\")]\n",
        "max_msa = \"512:1024\"\n",
        "\n",
        "max_msa_clusters, max_extra_msa = [int(x) for x in max_msa.split(\":\")]\n",
        "#@markdown Don't forget to hit `Runtime` -> `Run all` after updating form\n",
        "\n",
        "\n",
        "with open(f\"run.log\", \"w\") as text_file:\n",
        "    text_file.write(\"num_models=%s\\n\" % num_models)\n",
        "    text_file.write(\"use_amber=%s\\n\" % use_amber)\n",
        "    text_file.write(\"use_msa=%s\\n\" % use_msa)\n",
        "    text_file.write(\"msa_mode=%s\\n\" % msa_mode)\n",
        "    text_file.write(\"use_templates=%s\\n\" % use_templates)\n",
        "    text_file.write(\"homooligomer=%s\\n\" % homooligomer)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccGdbe_Pmt9",
        "cellView": "form"
      },
      "source": [
        "\n",
        "\n",
        "#@title Install software\n",
        "#@markdown Please execute this cell by pressing the _Play_ button \n",
        "#@markdown on the left.\n",
        "use_amber_relax = use_amber \n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "import jax\n",
        "if jax.local_devices()[0].platform == 'tpu':\n",
        "  raise RuntimeError('Colab TPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "elif jax.local_devices()[0].platform == 'cpu':\n",
        "  raise RuntimeError('Colab CPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "\n",
        "from IPython.utils import io\n",
        "import subprocess\n",
        "import tqdm.notebook\n",
        "\n",
        "GIT_REPO = 'https://github.com/deepmind/alphafold'\n",
        "SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar'\n",
        "PARAMS_DIR = './alphafold/data/params'\n",
        "PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(SOURCE_URL))\n",
        "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
        "\n",
        "# if not already installed\n",
        "try:\n",
        "  total = 100 if use_amber_relax else 55\n",
        "  with tqdm.notebook.tqdm(total=total, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    with io.capture_output() as captured:\n",
        "      if not os.path.isdir(\"alphafold\"):\n",
        "        %shell rm -rf alphafold\n",
        "        %shell git clone {GIT_REPO} alphafold\n",
        "        %shell (cd alphafold; git checkout 1e216f93f06aa04aa699562f504db1d02c3b704c --quiet)\n",
        "\n",
        "        # colabfold patches\n",
        "        %shell mkdir --parents tmp\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/pairmsa.py\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/protein.patch -P tmp/\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/config.patch -P tmp/\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/model.patch -P tmp/\n",
        "        %shell wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/modules.patch -P tmp/\n",
        "\n",
        "        # hhsuite + reformat.pl\n",
        "        %shell curl -fsSL https://github.com/soedinglab/hh-suite/releases/download/v3.3.0/hhsuite-3.3.0-SSE2-Linux.tar.gz | tar xz -C tmp/\n",
        "\n",
        "        # Apply multi-chain patch from Lim Heo @huhlim\n",
        "        %shell patch -u alphafold/alphafold/common/protein.py -i tmp/protein.patch\n",
        "        \n",
        "        # Apply patch to dynamically control number of recycles (idea from Ryan Kibler)\n",
        "        %shell patch -u alphafold/alphafold/model/model.py -i tmp/model.patch\n",
        "        %shell patch -u alphafold/alphafold/model/modules.py -i tmp/modules.patch\n",
        "        %shell patch -u alphafold/alphafold/model/config.py -i tmp/config.patch\n",
        "        pbar.update(4)\n",
        "\n",
        "        %shell pip3 install ./alphafold\n",
        "        pbar.update(5)\n",
        "      \n",
        "        # speedup from kaczmarj\n",
        "        %shell mkdir --parents \"{PARAMS_DIR}\"\n",
        "        %shell curl -fsSL \"{SOURCE_URL}\" | tar x -C \"{PARAMS_DIR}\"\n",
        "        pbar.update(14+27)\n",
        "\n",
        "        #######################################################################\n",
        "        %shell sudo apt install --quiet --yes hmmer\n",
        "        pbar.update(3)\n",
        "\n",
        "        # Install py3dmol.\n",
        "        %shell pip install py3dmol\n",
        "        pbar.update(1)\n",
        "\n",
        "        # Create a ramdisk to store a database chunk to make Jackhmmer run fast.\n",
        "        %shell sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "        %shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "        pbar.update(1)\n",
        "      else:\n",
        "        pbar.update(55)\n",
        "\n",
        "      if use_amber_relax:\n",
        "        if not os.path.isfile(\"stereo_chemical_props.txt\"):\n",
        "          # Install OpenMM and pdbfixer.\n",
        "          %shell rm -rf /opt/conda\n",
        "          %shell wget -q -P /tmp \\\n",
        "            https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
        "              && bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n",
        "              && rm /tmp/Miniconda3-latest-Linux-x86_64.sh\n",
        "          pbar.update(4)\n",
        "\n",
        "          PATH=%env PATH\n",
        "          %env PATH=/opt/conda/bin:{PATH}\n",
        "          %shell conda update -qy conda \\\n",
        "              && conda install -qy -c conda-forge \\\n",
        "                python=3.7 \\\n",
        "                openmm=7.5.1 \\\n",
        "                pdbfixer\n",
        "          pbar.update(40)\n",
        "\n",
        "          %shell wget -q -P /content \\\n",
        "            https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "          pbar.update(1)\n",
        "          %shell mkdir -p /content/alphafold/common\n",
        "          %shell cp -f /content/stereo_chemical_props.txt /content/alphafold/common\n",
        "\n",
        "          # Apply OpenMM patch.\n",
        "          %shell pushd /opt/conda/lib/python3.7/site-packages/ && \\\n",
        "              patch -p0 < /content/alphafold/docker/openmm.patch && \\\n",
        "              popd\n",
        "        else:\n",
        "          pbar.update(45)\n",
        "\n",
        "except subprocess.CalledProcessError:\n",
        "  print(captured)\n",
        "  raise\n",
        "\n",
        "########################################################################################\n",
        "# --- Python imports ---\n",
        "import colabfold as cf\n",
        "import pairmsa\n",
        "import sys\n",
        "import pickle\n",
        "if use_amber_relax:\n",
        "  sys.path.append('/opt/conda/lib/python3.7/site-packages')\n",
        "\n",
        "if \"/content/tmp/bin\" not in os.environ['PATH']:\n",
        "  os.environ['PATH'] += \":/content/tmp/bin:/content/tmp/scripts\"\n",
        "\n",
        "\n",
        "from urllib import request\n",
        "from concurrent import futures\n",
        "from google.colab import files\n",
        "import json\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "\n",
        "from alphafold.model import model\n",
        "from alphafold.model import config\n",
        "from alphafold.model import data\n",
        "\n",
        "from alphafold.data import parsers\n",
        "from alphafold.data import pipeline\n",
        "from alphafold.data.tools import jackhmmer\n",
        "\n",
        "from alphafold.common import protein\n",
        "\n",
        "if use_amber_relax:\n",
        "  from alphafold.relax import relax\n",
        "  from alphafold.relax import utils\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYApPElB30u",
        "cellView": "form"
      },
      "source": [
        "#@title Gather input features, predict structure\n",
        "\n",
        "def _placeholder_template_feats(num_templates_, num_res_):\n",
        "  return {\n",
        "      'template_aatype': np.zeros([num_templates_, num_res_, 22], np.float32),\n",
        "      'template_all_atom_masks': np.zeros([num_templates_, num_res_, 37, 3], np.float32),\n",
        "      'template_all_atom_positions': np.zeros([num_templates_, num_res_, 37], np.float32),\n",
        "      'template_domain_names': np.zeros([num_templates_], np.float32),\n",
        "      'template_sum_probs': np.zeros([num_templates_], np.float32),\n",
        "  }\n",
        "\n",
        "def parse_results(prediction_result, processed_feature_dict):\n",
        "  b_factors = prediction_result['plddt'][:,None] * prediction_result['structure_module']['final_atom_mask']\n",
        "  out = {\"unrelaxed_protein\": protein.from_prediction(processed_feature_dict, prediction_result, b_factors=b_factors),\n",
        "         \"plddt\": prediction_result['plddt'],\n",
        "         \"pLDDT\": prediction_result['plddt'].mean(),\n",
        "         \"dists\": prediction_result[\"distogram\"][\"bin_edges\"][prediction_result[\"distogram\"][\"logits\"].argmax(-1)],\n",
        "         \"adj\": jax.nn.softmax(prediction_result[\"distogram\"][\"logits\"])[:,:,prediction_result[\"distogram\"][\"bin_edges\"] < 8].sum(-1)}\n",
        "  if \"ptm\" in prediction_result:\n",
        "    out.update({\"pae\": prediction_result['predicted_aligned_error'],\n",
        "                \"pTMscore\": prediction_result['ptm']})\n",
        "  return out\n",
        "\n",
        "model_names = ['model_1', 'model_2', 'model_3', 'model_4', 'model_5'][:num_models]\n",
        "total = len(model_names) * num_samples\n",
        "if use_amber_relax:\n",
        "  if relax_all: total += total\n",
        "  else: total += 1\n",
        "\n",
        "\n",
        "### run\n",
        "for filename in os.listdir(input_dir):\n",
        "  jobname=os.path.splitext(filename)[0]\n",
        "  filepath = input_dir+\"/\"+filename\n",
        "  print(\"Running: \"+filepath)\n",
        "  with open(filepath) as f:\n",
        "    input_fasta_str = f.read()\n",
        "  (seqs, header) = pipeline.parsers.parse_fasta(input_fasta_str)\n",
        "  seq = seqs[0]\n",
        "  if os.path.isfile(result_dir+\"/\"+jobname+\".result.zip\"):\n",
        "    continue\n",
        "\n",
        "  # prediction directory\n",
        "  output_dir = 'prediction_' + cf.get_hash(seq)[:5]\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  # delete existing files in working directory\n",
        "  for f in os.listdir(output_dir):\n",
        "    os.remove(os.path.join(output_dir, f))\n",
        "\n",
        "  prefix = cf.get_hash(\"\".join(seq))\n",
        "  prefix = os.path.join('tmp',prefix)\n",
        "  print(f\"running mmseqs2\")\n",
        "  a3m_lines = cf.run_mmseqs2(seq, prefix, filter=True)\n",
        "  \n",
        "  # write a3m to output folder\n",
        "  with open(f\"{output_dir}/msa.a3m\",\"w\") as out_a3m:\n",
        "    out_a3m.write(a3m_lines)\n",
        "\n",
        "  msa, deletion_matrice = parsers.parse_a3m(a3m_lines)\n",
        "  \n",
        "  msas, deletion_matrices = [],[]\n",
        "  msas.append(msa)\n",
        "  deletion_matrices.append(deletion_matrice)\n",
        "  #############################\n",
        "  # homooligomerize\n",
        "  #############################\n",
        "  lengths = [len(seq) for seq in seqs]\n",
        "  msas_mod, deletion_matrices_mod = cf.homooligomerize_heterooligomer(msas, deletion_matrices,\n",
        "                                                                    lengths, homooligomers)\n",
        "\n",
        "  num_res = len(seq)\n",
        "  feature_dict = {}\n",
        "  feature_dict.update(pipeline.make_sequence_features(seq, 'test', num_res))\n",
        "  feature_dict.update(pipeline.make_msa_features(msas_mod, deletion_matrices=deletion_matrices_mod))\n",
        "  if not use_turbo:\n",
        "    feature_dict.update(_placeholder_template_feats(0, num_res))\n",
        "\n",
        "\n",
        "  ################################\n",
        "  # set chain breaks\n",
        "  ################################\n",
        "  Ls = []\n",
        "  for seq,h in zip(seq.split(\":\"),homooligomers):\n",
        "    Ls += [len(s) for s in seq.split(\"/\")] * h\n",
        "  Ls_plot = sum([[len(seq)]*h for seq,h in zip(seqs,homooligomers)],[])\n",
        "\n",
        "  \n",
        "  feature_dict['residue_index'] = cf.chain_break(feature_dict['residue_index'], Ls)\n",
        "  from string import ascii_uppercase\n",
        "\n",
        "  with tqdm.notebook.tqdm(total=total, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "    #######################################################################\n",
        "    # precompile model and recompile only if length changes\n",
        "    #######################################################################\n",
        "    if use_turbo:\n",
        "      name = \"model_5_ptm\" if use_ptm else \"model_5\"\n",
        "      N = len(feature_dict[\"msa\"])\n",
        "      L = len(feature_dict[\"residue_index\"])\n",
        "      compiled = (N, L, use_ptm, max_recycles, tol, num_ensemble, max_msa, is_training)\n",
        "      if \"COMPILED\" in dir():\n",
        "        if COMPILED != compiled: recompile = True\n",
        "      else: recompile = True\n",
        "      if recompile:\n",
        "        cf.clear_mem(\"gpu\")\n",
        "        cfg = config.model_config(name)      \n",
        "        cfg.data.common.max_extra_msa = min(N,max_extra_msa)\n",
        "        cfg.data.eval.max_msa_clusters = min(N,max_msa_clusters)\n",
        "        cfg.data.common.num_recycle = max_recycles\n",
        "        cfg.model.num_recycle = max_recycles\n",
        "        cfg.model.recycle_tol = tol\n",
        "        cfg.data.eval.num_ensemble = num_ensemble\n",
        "\n",
        "        params = data.get_model_haiku_params(name,'./alphafold/data')\n",
        "        model_runner = model.RunModel(cfg, params, is_training=is_training)\n",
        "        COMPILED = compiled\n",
        "        recompile = False\n",
        "\n",
        "    else:\n",
        "      cf.clear_mem(\"gpu\")\n",
        "      recompile = True\n",
        "\n",
        "    # cleanup\n",
        "    if \"outs\" in dir(): del outs\n",
        "    outs = {}\n",
        "    cf.clear_mem(\"cpu\")  \n",
        "\n",
        "    #######################################################################\n",
        "    for num, model_name in enumerate(model_names): # for each model\n",
        "      name = model_name+\"_ptm\" if use_ptm else model_name\n",
        "\n",
        "      # setup model and/or params\n",
        "      params = data.get_model_haiku_params(name, './alphafold/data')\n",
        "      if use_turbo:\n",
        "        for k in model_runner.params.keys():\n",
        "          model_runner.params[k] = params[k]\n",
        "      else:\n",
        "        cfg = config.model_config(name)\n",
        "        cfg.data.common.num_recycle = cfg.model.num_recycle = max_recycles\n",
        "        cfg.model.recycle_tol = tol\n",
        "        cfg.data.eval.num_ensemble = num_ensemble\n",
        "        model_runner = model.RunModel(cfg, params, is_training=is_training)\n",
        "\n",
        "      for seed in range(num_samples): # for each seed\n",
        "        # predict\n",
        "        key = f\"{name}_seed_{seed}\"\n",
        "        pbar.set_description(f'Running {key}')      \n",
        "        processed_feature_dict = model_runner.process_features(feature_dict, random_seed=seed)      \n",
        "        prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed),\"cpu\")\n",
        "        outs[key] = parse_results(prediction_result, processed_feature_dict)\n",
        "        \n",
        "        # report\n",
        "        pbar.update(n=1)\n",
        "        line = f\"{key} recycles:{r} tol:{t:.2f} pLDDT:{outs[key]['pLDDT']:.2f}\"\n",
        "        if use_ptm: line += f\" pTMscore:{outs[key]['pTMscore']:.2f}\"\n",
        "        print(line)\n",
        "        if show_images:\n",
        "          fig = cf.plot_protein(outs[key][\"unrelaxed_protein\"], Ls=Ls_plot, dpi=100)\n",
        "          plt.show()\n",
        "        if save_tmp_pdb:\n",
        "          tmp_pdb_path = os.path.join(output_dir,f'unranked_{key}_unrelaxed.pdb')\n",
        "          pdb_lines = protein.to_pdb(outs[key][\"unrelaxed_protein\"])\n",
        "          with open(tmp_pdb_path, 'w') as f: f.write(pdb_lines)\n",
        "\n",
        "\n",
        "        # cleanup\n",
        "        del processed_feature_dict, prediction_result\n",
        "\n",
        "      if use_turbo:\n",
        "        del params\n",
        "      else:\n",
        "        del params, model_runner, cfg\n",
        "        cf.clear_mem(\"gpu\")\n",
        "\n",
        "    # delete old files\n",
        "    for f in os.listdir(output_dir):\n",
        "      if \"rank\" in f:\n",
        "        os.remove(os.path.join(output_dir, f))\n",
        "\n",
        "    # Find the best model according to the mean pLDDT.\n",
        "    model_rank = list(outs.keys())\n",
        "    model_rank = [model_rank[i] for i in np.argsort([outs[x][rank_by] for x in model_rank])[::-1]]\n",
        "\n",
        "    # Write out the prediction\n",
        "    for n,key in enumerate(model_rank):\n",
        "      prefix = f\"rank_{n+1}_{key}\" \n",
        "      pred_output_path = os.path.join(output_dir,f'{prefix}_unrelaxed.pdb')\n",
        "      fig = cf.plot_protein(outs[key][\"unrelaxed_protein\"], Ls=Ls_plot, dpi=200)\n",
        "      plt.savefig(os.path.join(output_dir,f'{prefix}.png'), bbox_inches = 'tight')\n",
        "      plt.close(fig)\n",
        "\n",
        "      pdb_lines = protein.to_pdb(outs[key][\"unrelaxed_protein\"])\n",
        "      with open(pred_output_path, 'w') as f:\n",
        "        f.write(pdb_lines)\n",
        "      if use_amber_relax:\n",
        "        pbar.set_description(f'AMBER relaxation')\n",
        "        if relax_all or n == 0:\n",
        "          amber_relaxer = relax.AmberRelaxation(\n",
        "              max_iterations=0,\n",
        "              tolerance=2.39,\n",
        "              stiffness=10.0,\n",
        "              exclude_residues=[],\n",
        "              max_outer_iterations=20)\n",
        "          relaxed_pdb_lines, _, _ = amber_relaxer.process(prot=outs[key][\"unrelaxed_protein\"])        \n",
        "          pred_output_path = os.path.join(output_dir,f'{prefix}_relaxed.pdb')\n",
        "          with open(pred_output_path, 'w') as f:\n",
        "            f.write(relaxed_pdb_lines)\n",
        "          pbar.update(n=1)\n",
        "        \n",
        "  ############################################################\n",
        "  print(f\"model rank based on {rank_by}\")\n",
        "  for n,key in enumerate(model_rank):\n",
        "    print(f\"rank_{n+1}_{key} {rank_by}:{outs[key][rank_by]:.2f}\")\n",
        "    if use_ptm and save_pae_json:\n",
        "      pae = outs[key][\"pae\"]\n",
        "      max_pae = pae.max()\n",
        "      # Save pLDDT and predicted aligned error (if it exists)\n",
        "      pae_output_path = os.path.join(output_dir,f'rank_{n+1}_{key}_pae.json')\n",
        "      # Save predicted aligned error in the same format as the AF EMBL DB\n",
        "      rounded_errors = np.round(np.asarray(pae), decimals=1)\n",
        "      indices = np.indices((len(rounded_errors), len(rounded_errors))) + 1\n",
        "      indices_1 = indices[0].flatten().tolist()\n",
        "      indices_2 = indices[1].flatten().tolist()\n",
        "      pae_data = json.dumps([{\n",
        "          'residue1': indices_1,\n",
        "          'residue2': indices_2,\n",
        "          'distance': rounded_errors.flatten().tolist(),\n",
        "          'max_predicted_aligned_error': max_pae.item()\n",
        "      }],\n",
        "                            indent=None,\n",
        "                            separators=(',', ':'))\n",
        "      with open(pae_output_path, 'w') as f:\n",
        "        f.write(pae_data)\n",
        "  dpi =  100\n",
        "  if use_ptm:\n",
        "    print(\"predicted alignment error\")\n",
        "    cf.plot_paes([outs[k][\"pae\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
        "    plt.savefig(os.path.join(output_dir,f'predicted_alignment_error.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
        "\n",
        "  print(\"predicted contacts\")\n",
        "  cf.plot_adjs([outs[k][\"adj\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
        "  plt.savefig(os.path.join(output_dir,f'predicted_contacts.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
        "\n",
        "  print(\"predicted distogram\")\n",
        "  cf.plot_dists([outs[k][\"dists\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
        "  plt.savefig(os.path.join(output_dir,f'predicted_distogram.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
        "\n",
        "  print(\"predicted LDDT\")\n",
        "  cf.plot_plddts([outs[k][\"plddt\"] for k in model_rank], Ls=Ls_plot, dpi=dpi)\n",
        "  plt.savefig(os.path.join(output_dir,f'predicted_LDDT.png'), bbox_inches = 'tight', dpi=np.maximum(200,dpi))\n",
        "\n",
        "  %shell zip -FSrj $result_dir\"/\"$jobname\".result.zip\" \"run.log\" $output_dir\"/\"* "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUBLzB3C6WN"
      },
      "source": [
        "# Instructions <a name=\"Instructions\"></a>\n",
        "**Quick start**\n",
        "1. Change the runtime type to GPU at \"Runtime\" -> \"Change runtime type\" (improves speed).\n",
        "2. Paste your protein sequence in the input field below.\n",
        "3. Press \"Runtime\" -> \"Run all\".\n",
        "4. The pipeline consists of 10 steps. The currently running steps is indicated by a circle with a stop sign next to it.\n",
        "\n",
        "**Result zip file contents**\n",
        "\n",
        "1. PDB formatted structures sorted by avg. pIDDT. (relaxed, unrelaxed).\n",
        "2. Plots of the model quality.\n",
        "3. Plots of the MSA coverage.\n",
        "4. Parameter log file.\n",
        "5. A3M formatted input MSA.\n",
        "6. BibTeX file with citations for all used tools and databases.\n",
        "\n",
        "At the end of the job a download modal box will pop up with a `jobname.result.zip` file. Additionally, if the `save_to_google_drive` option was selected, the `jobname.result.zip` will be uploaded to your Google Drive.\n",
        "\n",
        "**Using a custom MSA as input**\n",
        "\n",
        "To predict the structure with a custom MSA (A3M formatted): (1) Change the msa_mode: to \"custom\", (2) Wait for an upload box to appear at the end of the \"Input Protein ...\" box. Upload your A3M. The first fasta entry of the A3M must be the query sequence without gaps.\n",
        "\n",
        "To generate good input MSAs the HHblits server can be used here: https://toolkit.tuebingen.mpg.de/tools/hhblits\n",
        "\n",
        "After submitting your query, click \"Query Template MSA\" -> \"Download Full A3M\". Download the a3m file and upload it to the notebook.\n",
        "\n",
        "**Troubleshooting**\n",
        "* Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "* Check your input sequence.\n",
        "\n",
        "**Known issues**\n",
        "* Colab assigns different types of GPUs with varying amount of memory. Some might have not enough memory to predict the structure.\n",
        "* Your browser can block the pop-up for downloading the result file. You can choose the `save_to_google_drive` option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
        "\n",
        "**Limitations**\n",
        "* MSAs: MMseqs2 is very precise and sensitive but might find less hits compared to HHblits/HMMer searched against BFD or Mgnify.\n",
        "* Computing resources: Our MMseqs2 API can probably handle ~20k requests per day.\n",
        "* For best results, we recommend using the full pipeline: https://github.com/deepmind/alphafold\n",
        "\n",
        "**Description of the plots**\n",
        "*   **Number of sequences per position** - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences.\n",
        "*   **Predicted lDDT per position** - model confidence (out of 100) at each position. Higher the better.\n",
        "*   **Predicted Alignment Error** - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. Lower the better.\n",
        "\n",
        "\n",
        "\n",
        "**Bugs**\n",
        "- If you encounter any bugs, please report the issue to https://github.com/sokrypton/ColabFold/issues\n",
        "\n",
        "\n",
        "**Acknowledgments**\n",
        "- We would like to thank the AlphaFold team for developing an excellent model and open sourcing the software. \n",
        "\n",
        "- A colab by Sergey Ovchinnikov ([@sokrypton](https://twitter.com/sokrypton)), Milot Mirdita ([@milot_mirdita](https://twitter.com/milot_mirdita)) and Martin Steinegger ([@thesteinegger](https://twitter.com/thesteinegger)).\n",
        "\n",
        "- Minkyung Baek ([@minkbaek](https://twitter.com/minkbaek)) and Yoshitaka Moriwaki ([@Ag_smith](https://twitter.com/Ag_smith)) for protein-complex prediction proof-of-concept in AlphaFold2.\n",
        "\n",
        "- Also, credit to [David Koes](https://github.com/dkoes) for his awesome [py3Dmol](https://3dmol.csb.pitt.edu/) plugin, without whom these notebooks would be quite boring!\n",
        "\n",
        "- For related notebooks see: [ColabFold](https://github.com/sokrypton/ColabFold)\n"
      ]
    }
  ]
}